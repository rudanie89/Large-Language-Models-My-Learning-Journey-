# Large-Language-Models-My-Learning-Journey
In this repository, I aim to collect and perhaps summarize some papers about Large Language Models (LLMs) that I am interested in and those that might be useful for my research. 
## Foundational Models
(1) Mixture of Experts (MoE)
(2) Multi-head Latent Attention (MLA)
(3) Rotary Positional Encodings (RoPE)
(4) Multi-token prediction (MTP)
(5) Supervised Fine-Tuning (SFT)
(6) Group Relative Policy Optimisation (GRPO)
## Reasoning in LLMs

[Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)

## DeepSeek papers 
[https://doi.org/10.48550/arXiv.2501.12948]

## Scaling up Test-Time Compute with Latent Reasoning
[https://doi.org/10.48550/arXiv.2502.05171]
## LLMs for misinformation problems
