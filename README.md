# Large-Language-Models-My-Learning-Journey
In this repository, I aim to collect and perhaps summarize some papers/resources about Large Language Models (LLMs) that I am interested in and those that are useful for my research. 
## Foundational Models
(1) Mixture of Experts (MoE)

(2) Multi-head Latent Attention (MLA)

(3) Rotary Positional Encodings (RoPE)

(4) Multi-token prediction (MTP)

(5) Supervised Fine-Tuning (SFT)

(6) Group Relative Policy Optimisation (GRPO)
[

## Reasoning in LLMs

[Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)

## DeepSeek papers 
[https://doi.org/10.48550/arXiv.2501.12948]

## Scaling up Test-Time Compute in LLMs

[Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking](https://doi.org/10.48550/arXiv.2503.19855)
[Scaling up Test-Time Compute with Latent Reasoning:A Recurrent Depth Approach](https://doi.org/10.48550/arXiv.2502.05171)
[Training Large LanguageModelstoReasonina ContinuousLatentSpace](https://doi.org/10.48550/arXiv.2412.06769)

## LLMs for misinformation problems
